{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 층과 출력 층간의 여러 개의(깊은) hidden layer를 가지고 있는 인공 신경망\n",
    "- 수백 개의 뉴런을 가진(wide) hidden layer가 10개 이상(deep) 연결된 경우\n",
    " - 10이 deep의 기준이 아니고 많다는 것을 의미하기 위해 표기함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/dnn.png\" width=\"1200\" height=\"4600\">\n",
    "(그림 출처 : google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "심층 신경망을 훈련해야 할 때 겪어야 하는 문제들\n",
    " - Vanishing Gradient(그래디언트 소실)\n",
    " - Exploding Gradient(그래디언트 폭주)\n",
    " - 훈련 속도 : 이런 대규모의 신경망에서는 훈련이 극단적으로 느려짐\n",
    " - Overfitting : 수백만 개의 parameter를 가진 모델은 훈련 세트에 과대적합될 위험이 매우 큼 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vanishing Gradient and Exploding Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역전파 알고리즘은 출력층에서 입력층으로 오차 gradient를 전파시키면서 진행됩니다. 알고리즘이 신경망의 모든 파라미터에 대한 오차 함수의 gradient를 계산하면 경사 하강법 단계에서 이 gradient를 사용하여 각 파라미터를 수정합니다.\n",
    "\n",
    "알고리즘이 하위층으로 진행됨에 따라 gradient는 점점 작아지는 경우가 많습니다. Gradient가 엄청 작으면, 파라미터가 변경되는 양은 많지 않을 것입니다. 결국 경사 하강법이 하위층의 연결 가중치를 실제 변경되지 않은 채로 둔다면 훈련이 좋은 솔루션(우리가 원하는 솔루션)으로 수렴되지 않습니다. 이 문제를 그래디언트 소실(Vanishing Gradient)이라고 한다.\n",
    "\n",
    "어떤 경우에는 반대 현상이 일어날 수 있습니다.\n",
    "\n",
    "Gradient가 점점 커져 여러 개의 층이 비정상적으로 큰 가중치로 갱신되면 알고리즘은 발산(diverse)하게 됩니다. 이 문제를 그래디언트 폭주(Exploding Gradient)라고 하며 순환 신경망에서 주로 나타납니다.\n",
    "\n",
    "이런 좋지 않은 학습 패턴이 꽤 오랫동안 경험적으로 관측되어 왔지만(이것이 심층 신경망이 오랫동안 거의 방치되었던 이유 중 하나입니다.) 2010년 즈음에 와서야 이에 대한 이해가 많이 진전되었습니다. Xavier Glorot과 Yoshua Bengio는 논문을 통해 몇 가지 주요한 발견을 발표했는데, 그 중에는 많이 사용되는 로지스틱 시그모이드 활성화 함수와 그 당시 가장 인기 있었던 가중치 초기화 방법의 조합이 있습니다. 이 초기화 방법은 평균이 0이고 표준편차가 1인 정규분포를 사용한 무작위 초기화입니다. 즉 이 활성화 함수와 초기화 방식을 사용했을 때 각 층에서 출력의 분산이 입력의 분산보다 더 크다는 것을 밝혔습니다. 신경망의 위쪽으로 갈수록 층을 지날 때마다 분산이 계속 커져 가장 높은 층에서는 활성화 함수가 0이나 1로 수렴합니다. 이는 로지스틱 함수의 평군이 0이 아니고 0.5라는 사실 때문에 더 나빠집니다. (하이퍼볼릭 탄젠트 함수는 평군이 0이므로 깊은 신경망에서는 로지스틱 함수보다 조금 더 낫습니다.) 로지스틱 함수는 항상 양수를 출력하므로 출력의 가중치 합이 입력보다 커질 가능성이 높습니다. 이를 편향 이동(bias shift)이라고도 부릅니다. 로지스틱 활성화 함수를 보면 입력의 절대값이 크면 0이나 1로 수렴해서 기울기가 0에 매우 가까워지는 것을 알 수 있습니다. 그래서 역전파가 될 때 사실상 신경망에서 전파시킬 그래디언트가 거의 없고 조금 있는 그래디언트는 최상위층에서부터 역전파가 진행됨에 따라 점차 약해져서 실제로 아래쪽 층에는 아무것도 도달하지 않게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
